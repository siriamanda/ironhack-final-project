{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('clothing_reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>recommended</th>\n",
       "      <th>length</th>\n",
       "      <th>polarity</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>no_stop</th>\n",
       "      <th>no_num</th>\n",
       "      <th>words_removed</th>\n",
       "      <th>token</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>absolutely wonderful  silky and sexy and comfo...</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "      <td>[('absolutely', 'RB'), ('wonderful', 'JJ'), ('...</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>1</td>\n",
       "      <td>303</td>\n",
       "      <td>0.339583</td>\n",
       "      <td>love this dress! it's sooo pretty. i happened ...</td>\n",
       "      <td>love this dress its sooo pretty i happened to ...</td>\n",
       "      <td>love dress sooo pretty happened find store im ...</td>\n",
       "      <td>love dress sooo pretty happened find store im ...</td>\n",
       "      <td>love sooo pretty happened find store glad bc n...</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happened', 'find',...</td>\n",
       "      <td>[('love', 'VB'), ('sooo', 'NN'), ('pretty', 'R...</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happen', 'find', '...</td>\n",
       "      <td>love sooo pretty happen find store glad bc nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.073675</td>\n",
       "      <td>i had such high hopes for this dress and reall...</td>\n",
       "      <td>i had such high hopes for this dress and reall...</td>\n",
       "      <td>high hopes dress really wanted work initially ...</td>\n",
       "      <td>high hopes dress really wanted work initially ...</td>\n",
       "      <td>high hopes really wanted work initially ordere...</td>\n",
       "      <td>['high', 'hopes', 'really', 'wanted', 'work', ...</td>\n",
       "      <td>[('high', 'JJ'), ('hopes', 'NNS'), ('really', ...</td>\n",
       "      <td>['high', 'hope', 'really', 'want', 'work', 'in...</td>\n",
       "      <td>high hope really want work initially order pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>i love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>i love love love this jumpsuit its fun flirty ...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "      <td>[('love', 'VB'), ('love', 'NN'), ('love', 'NN'...</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0.512891</td>\n",
       "      <td>this shirt is very flattering to all due to th...</td>\n",
       "      <td>this shirt is very flattering to all due to th...</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>flattering due adjustable front tie perfect le...</td>\n",
       "      <td>['flattering', 'due', 'adjustable', 'front', '...</td>\n",
       "      <td>[('flattering', 'VBG'), ('due', 'JJ'), ('adjus...</td>\n",
       "      <td>['flatter', 'due', 'adjustable', 'front', 'tie...</td>\n",
       "      <td>flatter due adjustable front tie perfect lengt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  recommended  \\\n",
       "0           0  Absolutely wonderful - silky and sexy and comf...            1   \n",
       "1           1  Love this dress!  it's sooo pretty.  i happene...            1   \n",
       "2           2  I had such high hopes for this dress and reall...            0   \n",
       "3           3  I love, love, love this jumpsuit. it's fun, fl...            1   \n",
       "4           4  This shirt is very flattering to all due to th...            1   \n",
       "\n",
       "   length  polarity                                              lower  \\\n",
       "0      53  0.633333  absolutely wonderful - silky and sexy and comf...   \n",
       "1     303  0.339583  love this dress! it's sooo pretty. i happened ...   \n",
       "2     500  0.073675  i had such high hopes for this dress and reall...   \n",
       "3     124  0.550000  i love, love, love this jumpsuit. it's fun, fl...   \n",
       "4     192  0.512891  this shirt is very flattering to all due to th...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  absolutely wonderful  silky and sexy and comfo...   \n",
       "1  love this dress its sooo pretty i happened to ...   \n",
       "2  i had such high hopes for this dress and reall...   \n",
       "3  i love love love this jumpsuit its fun flirty ...   \n",
       "4  this shirt is very flattering to all due to th...   \n",
       "\n",
       "                                             no_stop  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love dress sooo pretty happened find store im ...   \n",
       "2  high hopes dress really wanted work initially ...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  shirt flattering due adjustable front tie perf...   \n",
       "\n",
       "                                              no_num  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love dress sooo pretty happened find store im ...   \n",
       "2  high hopes dress really wanted work initially ...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  shirt flattering due adjustable front tie perf...   \n",
       "\n",
       "                                       words_removed  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love sooo pretty happened find store glad bc n...   \n",
       "2  high hopes really wanted work initially ordere...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  flattering due adjustable front tie perfect le...   \n",
       "\n",
       "                                               token  \\\n",
       "0  ['absolutely', 'wonderful', 'silky', 'sexy', '...   \n",
       "1  ['love', 'sooo', 'pretty', 'happened', 'find',...   \n",
       "2  ['high', 'hopes', 'really', 'wanted', 'work', ...   \n",
       "3  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...   \n",
       "4  ['flattering', 'due', 'adjustable', 'front', '...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [('absolutely', 'RB'), ('wonderful', 'JJ'), ('...   \n",
       "1  [('love', 'VB'), ('sooo', 'NN'), ('pretty', 'R...   \n",
       "2  [('high', 'JJ'), ('hopes', 'NNS'), ('really', ...   \n",
       "3  [('love', 'VB'), ('love', 'NN'), ('love', 'NN'...   \n",
       "4  [('flattering', 'VBG'), ('due', 'JJ'), ('adjus...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['absolutely', 'wonderful', 'silky', 'sexy', '...   \n",
       "1  ['love', 'sooo', 'pretty', 'happen', 'find', '...   \n",
       "2  ['high', 'hope', 'really', 'want', 'work', 'in...   \n",
       "3  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...   \n",
       "4  ['flatter', 'due', 'adjustable', 'front', 'tie...   \n",
       "\n",
       "                                              joined  \n",
       "0        absolutely wonderful silky sexy comfortable  \n",
       "1  love sooo pretty happen find store glad bc nev...  \n",
       "2  high hope really want work initially order pet...  \n",
       "3  love love love jumpsuit fun flirty fabulous ev...  \n",
       "4  flatter due adjustable front tie perfect lengt...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18540\n",
       "0     4101\n",
       "Name: recommended, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = reviews_df[['recommended', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "22636    1\n",
       "22637    1\n",
       "22638    0\n",
       "22639    1\n",
       "22640    1\n",
       "Name: recommended, Length: 22641, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df['recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data frames for 'positive' and 'negative' words\n",
    "\n",
    "pos_df = analysis_df[analysis_df['recommended'] == 1]\n",
    "neg_df = analysis_df[analysis_df['recommended'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommended</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happen', 'find', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['flatter', 'due', 'adjustable', 'front', 'tie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>['aded', 'basket', 'hte', 'last', 'mintue', 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recommended                                         lemmatized\n",
       "0            1  ['absolutely', 'wonderful', 'silky', 'sexy', '...\n",
       "1            1  ['love', 'sooo', 'pretty', 'happen', 'find', '...\n",
       "3            1  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...\n",
       "4            1  ['flatter', 'due', 'adjustable', 'front', 'tie...\n",
       "6            1  ['aded', 'basket', 'hte', 'last', 'mintue', 's..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommended</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happen', 'find', '...</td>\n",
       "      <td>0.318750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['flatter', 'due', 'adjustable', 'front', 'tie...</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>['aded', 'basket', 'hte', 'last', 'mintue', 's...</td>\n",
       "      <td>0.075625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recommended                                         lemmatized  polarity\n",
       "0            1  ['absolutely', 'wonderful', 'silky', 'sexy', '...  0.633333\n",
       "1            1  ['love', 'sooo', 'pretty', 'happen', 'find', '...  0.318750\n",
       "3            1  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...  0.500000\n",
       "4            1  ['flatter', 'due', 'adjustable', 'front', 'tie...  0.458333\n",
       "6            1  ['aded', 'basket', 'hte', 'last', 'mintue', 's...  0.075625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polarity of positive dataframe\n",
    "\n",
    "pos_df['polarity'] = pos_df['lemmatized'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "pos_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    18540.000000\n",
       "mean         0.267034\n",
       "std          0.179089\n",
       "min         -0.700000\n",
       "25%          0.149148\n",
       "50%          0.253244\n",
       "75%          0.373469\n",
       "max          1.000000\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df['polarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommended</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>['high', 'hope', 'really', 'want', 'work', 'in...</td>\n",
       "      <td>0.045530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>['love', 'tracy', 'reese', 'dress', 'petite', ...</td>\n",
       "      <td>0.106250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>['run', 'small', 'esp', 'zipper', 'area', 'run...</td>\n",
       "      <td>-0.078571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>['first', 'pullover', 'style', 'side', 'zipper...</td>\n",
       "      <td>-0.058095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>['love', 'didnt', 'really', 'long', 'purchase'...</td>\n",
       "      <td>0.207653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    recommended                                         lemmatized  polarity\n",
       "2             0  ['high', 'hope', 'really', 'want', 'work', 'in...  0.045530\n",
       "5             0  ['love', 'tracy', 'reese', 'dress', 'petite', ...  0.106250\n",
       "10            0  ['run', 'small', 'esp', 'zipper', 'area', 'run... -0.078571\n",
       "22            0  ['first', 'pullover', 'style', 'side', 'zipper... -0.058095\n",
       "25            0  ['love', 'didnt', 'really', 'long', 'purchase'...  0.207653"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polarity of negative dataframe\n",
    "\n",
    "neg_df['polarity'] = neg_df['lemmatized'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "neg_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4101.000000\n",
       "mean        0.115536\n",
       "std         0.183357\n",
       "min        -1.000000\n",
       "25%         0.002500\n",
       "50%         0.110897\n",
       "75%         0.220635\n",
       "max         0.900000\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mean polarity of the 'negative' reviews was 0.1 (which actually is positive)\n",
    "# The highest polarity on the negative dataframe is 0.9 which indicate that there might be a few positive reviews \n",
    "# which has been rated 'not recommended'\n",
    "\n",
    "neg_df['polarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two balanced samples for the analysis\n",
    "\n",
    "pos_sample = pos_df.sample(4000)\n",
    "neg_sample = neg_df.sample(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000.000000\n",
       "mean        0.268426\n",
       "std         0.178669\n",
       "min        -0.333333\n",
       "25%         0.149256\n",
       "50%         0.254056\n",
       "75%         0.371317\n",
       "max         1.000000\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sample['polarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000.000000\n",
       "mean        0.115380\n",
       "std         0.183614\n",
       "min        -1.000000\n",
       "25%         0.002177\n",
       "50%         0.110714\n",
       "75%         0.221136\n",
       "max         0.900000\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sample['polarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1: \n",
    "\n",
    "H0: There is no significant difference in the mean polarity of the 'positive sample' and the mean polarity of all 'positive' reviews.\n",
    "\n",
    "HA: There is significant difference in the mean polarity of the 'positive sample' and the mean polarity of all 'positive' reviews.\n",
    "\n",
    "Since the variance of the population (the full positive sample) is known we will use a Z-test.\n",
    "\n",
    "Rejection Region for Two-Tailed Z Test (H0: μ ≠ μ 0 ) with α =0.05. \n",
    "The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_n = 4000\n",
    "p_pop_mean = 0.267034\n",
    "p_sample_mean = 0.270990\n",
    "p_pop_std = 0.178884\n",
    "\n",
    "p_statistic = (p_sample_mean - p_pop_mean) / (p_pop_std / math.sqrt(p_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z statistic is:  1.3986684581769366\n"
     ]
    }
   ],
   "source": [
    "print('The Z statistic is: ', p_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 1.39 < 1.96 we do not reject the null hypothesis at 95% significance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2: \n",
    "\n",
    "H0: There is no significant difference in the mean polarity of the 'negative sample' and the mean polarity of all 'negative' reviews.\n",
    "\n",
    "HA: There is significant difference in the mean polarity of the 'negative sample' and the mean polarity of all 'negative' reviews.\n",
    "\n",
    "The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_n = 4000\n",
    "n_pop_mean = 0.115536\n",
    "n_sample_mean = 0.115348\n",
    "n_pop_std = 0.183100\n",
    "\n",
    "n_statistic = (n_sample_mean - n_pop_mean) / (n_pop_std / math.sqrt(n_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z statistic is:  -0.06493808848843642\n"
     ]
    }
   ],
   "source": [
    "print('The Z statistic is: ', n_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since -0.06 > -1.96 we do not reject the null hypothesis at 95% significance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no significant difference in the polarity we can assume that the samples are representative\n",
    "\n",
    "# Create a new dataframe including both samples\n",
    "\n",
    "new_df = pd.concat([pos_sample, neg_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommended</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['order', 'xl', 'perfect', 'drape', 'flatterin...</td>\n",
       "      <td>0.523611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['look', 'great', 'give', 'slender', 'appearan...</td>\n",
       "      <td>0.544444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['tie', 'two', 'different', 'material', 'soft'...</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['gorgeous', 'absolutely', 'stunning', 'look',...</td>\n",
       "      <td>0.223852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['yearold', 'daughter', 'fell', 'love', 'saw',...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0</td>\n",
       "      <td>['extremely', 'disappointed', 'arrive', 'nothi...</td>\n",
       "      <td>-0.245476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>['cute', 'seem', 'high', 'quality', 'love', 'k...</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0</td>\n",
       "      <td>['texture', 'didnt', 'meet', 'expectation', 'u...</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>['lovely', 'cut', 'pretty', 'creamy', 'great',...</td>\n",
       "      <td>0.271795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0</td>\n",
       "      <td>['initially', 'saw', 'month', 'ago', 'fell', '...</td>\n",
       "      <td>0.202797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      recommended                                         lemmatized  polarity\n",
       "0               1  ['order', 'xl', 'perfect', 'drape', 'flatterin...  0.523611\n",
       "1               1  ['look', 'great', 'give', 'slender', 'appearan...  0.544444\n",
       "2               1  ['tie', 'two', 'different', 'material', 'soft'...  0.150000\n",
       "3               1  ['gorgeous', 'absolutely', 'stunning', 'look',...  0.223852\n",
       "4               1  ['yearold', 'daughter', 'fell', 'love', 'saw',...  0.166667\n",
       "...           ...                                                ...       ...\n",
       "7995            0  ['extremely', 'disappointed', 'arrive', 'nothi... -0.245476\n",
       "7996            0  ['cute', 'seem', 'high', 'quality', 'love', 'k...  0.290000\n",
       "7997            0  ['texture', 'didnt', 'meet', 'expectation', 'u...  0.050000\n",
       "7998            0  ['lovely', 'cut', 'pretty', 'creamy', 'great',...  0.271795\n",
       "7999            0  ['initially', 'saw', 'month', 'ago', 'fell', '...  0.202797\n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df['lemmatized']\n",
    "y = new_df['recommended']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001    ['love', 'asymmetrical', 'zipper', 'soft', 'co...\n",
       "7360    ['like', 'allbut', 'look', 'prior', 'purchasin...\n",
       "5234    ['want', 'love', 'excite', 'ordered', 'online'...\n",
       "7390    ['adorable', 'love', 'lace', 'detail', 'side',...\n",
       "6841    ['curvy', 'stylish', 'love', 'pocket', 'didnt'...\n",
       "                              ...                        \n",
       "4931    ['head', 'romper', 'pant', 'cant', 'tell', 'pi...\n",
       "3264    ['feel', 'hang', 'nicely', 'versatile', 'purch...\n",
       "1653    ['ordered', 'neutral', 'p', 'hem', 'lb', 'wear...\n",
       "2607    ['buy', 'dress', 'online', 'attempt', 'find', ...\n",
       "2732    ['month', 'post', 'baby', 'still', 'body', 'se...\n",
       "Name: lemmatized, Length: 6400, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069    ['tried', 'fluke', 'work', 'retailer', 'like',...\n",
       "1675    ['love', 'get', 'blue', 'color', 'deep', 'beau...\n",
       "6385    ['love', 'tracy', 'reese', 'dress', 'petite', ...\n",
       "543     ['hard', 'time', 'find', 'slack', 'step', 'gre...\n",
       "3213    ['love', 'charlie', 'trouser', 'must', 'pair',...\n",
       "                              ...                        \n",
       "7716    ['much', 'buy', 'retailer', 'first', 'review',...\n",
       "4766    ['design', 'nice', 'texture', 'feel', 'comfort...\n",
       "4096    ['much', 'potential', 'love', 'pattern', 'neck...\n",
       "1595    ['agree', 'good', 'review', 'love', 'feel', 'v...\n",
       "5023    ['particularly', 'large', 'thigh', 'pair', 'pa...\n",
       "Name: lemmatized, Length: 1600, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001    1\n",
       "7360    0\n",
       "5234    0\n",
       "7390    0\n",
       "6841    0\n",
       "       ..\n",
       "4931    0\n",
       "3264    1\n",
       "1653    1\n",
       "2607    1\n",
       "2732    1\n",
       "Name: recommended, Length: 6400, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069    1\n",
       "1675    1\n",
       "6385    0\n",
       "543     1\n",
       "3213    1\n",
       "       ..\n",
       "7716    0\n",
       "4766    0\n",
       "4096    0\n",
       "1595    1\n",
       "5023    0\n",
       "Name: recommended, Length: 1600, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a model pipeline including the vectorizer, TfidfTransformer and classifier\n",
    "\n",
    "model_nb = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),     # ignore terms that have a document frequency lower than 5\n",
    "                   ('tfidf', TfidfTransformer()),                                    # include unigrams and bigrams\n",
    "                   ('clf', MultinomialNB()),\n",
    "                   ])\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is 0.863125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       790\n",
      "           1       0.88      0.85      0.86       810\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.86      0.86      0.86      1600\n",
      "weighted avg       0.86      0.86      0.86      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The model accuracy is %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[653 137]\n",
      " [129 681]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(ytest, pred_y)\n",
    "cnf_matrix\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = \"YlGnBu\" ,fmt = 'g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y = 1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the results from the Multinomial Naive Bayes model with some other common models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', LogisticRegression()),\n",
    "                   ])\n",
    "\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.861875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       790\n",
      "           1       0.86      0.86      0.86       810\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.86      0.86      0.86      1600\n",
      "weighted avg       0.86      0.86      0.86      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[680 110]\n",
      " [111 699]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', DecisionTreeClassifier()),\n",
    "                   ])\n",
    "\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.741875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73       790\n",
      "           1       0.74      0.76      0.75       810\n",
      "\n",
      "    accuracy                           0.74      1600\n",
      "   macro avg       0.74      0.74      0.74      1600\n",
      "weighted avg       0.74      0.74      0.74      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[568 222]\n",
      " [191 619]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', RandomForestClassifier()),\n",
    "                   ])\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.844375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       790\n",
      "           1       0.84      0.85      0.85       810\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.84      0.84      0.84      1600\n",
      "weighted avg       0.84      0.84      0.84      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[659 131]\n",
      " [118 692]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', svm.SVC()),\n",
    "                   ])\n",
    "\n",
    "model_svm.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.86125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       790\n",
      "           1       0.87      0.86      0.86       810\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.86      0.86      0.86      1600\n",
      "weighted avg       0.86      0.86      0.86      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[685 105]\n",
      " [117 693]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same test and train set, Naive Bayes, SVM and Logistic Regression all had an accuray of 86%. Followed by Random Forest at 84%. Decision Tree preformed worst at 74%.\n",
    "\n",
    "This indicate that Naive Bayes is a good predictor of sentiment in reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

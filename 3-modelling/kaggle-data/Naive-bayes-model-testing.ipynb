{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('clothing_reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>recommended</th>\n",
       "      <th>length</th>\n",
       "      <th>polarity</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>no_stop</th>\n",
       "      <th>no_num</th>\n",
       "      <th>words_removed</th>\n",
       "      <th>token</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>absolutely wonderful  silky and sexy and comfo...</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "      <td>[('absolutely', 'RB'), ('wonderful', 'JJ'), ('...</td>\n",
       "      <td>['absolutely', 'wonderful', 'silky', 'sexy', '...</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>1</td>\n",
       "      <td>303</td>\n",
       "      <td>0.339583</td>\n",
       "      <td>love this dress! it's sooo pretty. i happened ...</td>\n",
       "      <td>love this dress its sooo pretty i happened to ...</td>\n",
       "      <td>love dress sooo pretty happened find store im ...</td>\n",
       "      <td>love dress sooo pretty happened find store im ...</td>\n",
       "      <td>love sooo pretty happened find store glad bc n...</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happened', 'find',...</td>\n",
       "      <td>[('love', 'VB'), ('sooo', 'NN'), ('pretty', 'R...</td>\n",
       "      <td>['love', 'sooo', 'pretty', 'happen', 'find', '...</td>\n",
       "      <td>love sooo pretty happen find store glad bc nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.073675</td>\n",
       "      <td>i had such high hopes for this dress and reall...</td>\n",
       "      <td>i had such high hopes for this dress and reall...</td>\n",
       "      <td>high hopes dress really wanted work initially ...</td>\n",
       "      <td>high hopes dress really wanted work initially ...</td>\n",
       "      <td>high hopes really wanted work initially ordere...</td>\n",
       "      <td>['high', 'hopes', 'really', 'wanted', 'work', ...</td>\n",
       "      <td>[('high', 'JJ'), ('hopes', 'NNS'), ('really', ...</td>\n",
       "      <td>['high', 'hope', 'really', 'want', 'work', 'in...</td>\n",
       "      <td>high hope really want work initially order pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>i love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>i love love love this jumpsuit its fun flirty ...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "      <td>[('love', 'VB'), ('love', 'NN'), ('love', 'NN'...</td>\n",
       "      <td>['love', 'love', 'love', 'jumpsuit', 'fun', 'f...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0.512891</td>\n",
       "      <td>this shirt is very flattering to all due to th...</td>\n",
       "      <td>this shirt is very flattering to all due to th...</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>flattering due adjustable front tie perfect le...</td>\n",
       "      <td>['flattering', 'due', 'adjustable', 'front', '...</td>\n",
       "      <td>[('flattering', 'VBG'), ('due', 'JJ'), ('adjus...</td>\n",
       "      <td>['flatter', 'due', 'adjustable', 'front', 'tie...</td>\n",
       "      <td>flatter due adjustable front tie perfect lengt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  recommended  \\\n",
       "0           0  Absolutely wonderful - silky and sexy and comf...            1   \n",
       "1           1  Love this dress!  it's sooo pretty.  i happene...            1   \n",
       "2           2  I had such high hopes for this dress and reall...            0   \n",
       "3           3  I love, love, love this jumpsuit. it's fun, fl...            1   \n",
       "4           4  This shirt is very flattering to all due to th...            1   \n",
       "\n",
       "   length  polarity                                              lower  \\\n",
       "0      53  0.633333  absolutely wonderful - silky and sexy and comf...   \n",
       "1     303  0.339583  love this dress! it's sooo pretty. i happened ...   \n",
       "2     500  0.073675  i had such high hopes for this dress and reall...   \n",
       "3     124  0.550000  i love, love, love this jumpsuit. it's fun, fl...   \n",
       "4     192  0.512891  this shirt is very flattering to all due to th...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  absolutely wonderful  silky and sexy and comfo...   \n",
       "1  love this dress its sooo pretty i happened to ...   \n",
       "2  i had such high hopes for this dress and reall...   \n",
       "3  i love love love this jumpsuit its fun flirty ...   \n",
       "4  this shirt is very flattering to all due to th...   \n",
       "\n",
       "                                             no_stop  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love dress sooo pretty happened find store im ...   \n",
       "2  high hopes dress really wanted work initially ...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  shirt flattering due adjustable front tie perf...   \n",
       "\n",
       "                                              no_num  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love dress sooo pretty happened find store im ...   \n",
       "2  high hopes dress really wanted work initially ...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  shirt flattering due adjustable front tie perf...   \n",
       "\n",
       "                                       words_removed  \\\n",
       "0        absolutely wonderful silky sexy comfortable   \n",
       "1  love sooo pretty happened find store glad bc n...   \n",
       "2  high hopes really wanted work initially ordere...   \n",
       "3  love love love jumpsuit fun flirty fabulous ev...   \n",
       "4  flattering due adjustable front tie perfect le...   \n",
       "\n",
       "                                               token  \\\n",
       "0  ['absolutely', 'wonderful', 'silky', 'sexy', '...   \n",
       "1  ['love', 'sooo', 'pretty', 'happened', 'find',...   \n",
       "2  ['high', 'hopes', 'really', 'wanted', 'work', ...   \n",
       "3  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...   \n",
       "4  ['flattering', 'due', 'adjustable', 'front', '...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [('absolutely', 'RB'), ('wonderful', 'JJ'), ('...   \n",
       "1  [('love', 'VB'), ('sooo', 'NN'), ('pretty', 'R...   \n",
       "2  [('high', 'JJ'), ('hopes', 'NNS'), ('really', ...   \n",
       "3  [('love', 'VB'), ('love', 'NN'), ('love', 'NN'...   \n",
       "4  [('flattering', 'VBG'), ('due', 'JJ'), ('adjus...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['absolutely', 'wonderful', 'silky', 'sexy', '...   \n",
       "1  ['love', 'sooo', 'pretty', 'happen', 'find', '...   \n",
       "2  ['high', 'hope', 'really', 'want', 'work', 'in...   \n",
       "3  ['love', 'love', 'love', 'jumpsuit', 'fun', 'f...   \n",
       "4  ['flatter', 'due', 'adjustable', 'front', 'tie...   \n",
       "\n",
       "                                              joined  \n",
       "0        absolutely wonderful silky sexy comfortable  \n",
       "1  love sooo pretty happen find store glad bc nev...  \n",
       "2  high hope really want work initially order pet...  \n",
       "3  love love love jumpsuit fun flirty fabulous ev...  \n",
       "4  flatter due adjustable front tie perfect lengt...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18540\n",
       "0     4101\n",
       "Name: recommended, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['recommended'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = reviews_df[['recommended', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "22636    1\n",
       "22637    1\n",
       "22638    0\n",
       "22639    1\n",
       "22640    1\n",
       "Name: recommended, Length: 22641, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df['recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = analysis_df['lemmatized']\n",
    "y = analysis_df['recommended']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Vectorizer + TFID transformer + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = 5, ngram_range = (1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB()),\n",
    "                   ])\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8330757341576507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.15      0.25       877\n",
      "           1       0.83      1.00      0.91      3652\n",
      "\n",
      "    accuracy                           0.83      4529\n",
      "   macro avg       0.89      0.57      0.58      4529\n",
      "weighted avg       0.85      0.83      0.78      4529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Vectorizer + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,2)) \n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', classifier)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = sentiment_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8330757341576507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.16      0.27       877\n",
      "           1       0.83      1.00      0.91      3652\n",
      "\n",
      "    accuracy                           0.83      4529\n",
      "   macro avg       0.86      0.58      0.59      4529\n",
      "weighted avg       0.84      0.83      0.78      4529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Balanced Samples. Vectorizer + TFID transformer + Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = analysis_df[analysis_df['recommended'] == 1]\n",
    "neg_df = analysis_df[analysis_df['recommended'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample = pos_df.sample(4000)\n",
    "neg_sample = neg_df.sample(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([pos_sample, neg_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommended</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['adorable', 'love', 'however', 'pink', 'pink'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'soft', 'comfortable', 'wore', 'casua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['buy', 'fall', 'wedding', 'absolutely', 'gorg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['amaze', 'swimsuit', 'hunt', 'perfect', 'piec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['wonderful', 'trouser', 'pant', 'love', 'fit'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0</td>\n",
       "      <td>['want', 'love', 'kimonoit', 'beautiful', 'wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>['beautiful', 'springsummer', 'quality', 'mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0</td>\n",
       "      <td>['think', 'clean', 'look', 'theoryvince', 'men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>['didnt', 'realize', 'sequin', 'design', 'keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0</td>\n",
       "      <td>['hei', 'hei', 'favorite', 'line', 'medium', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      recommended                                         lemmatized\n",
       "0               1  ['adorable', 'love', 'however', 'pink', 'pink'...\n",
       "1               1  ['love', 'soft', 'comfortable', 'wore', 'casua...\n",
       "2               1  ['buy', 'fall', 'wedding', 'absolutely', 'gorg...\n",
       "3               1  ['amaze', 'swimsuit', 'hunt', 'perfect', 'piec...\n",
       "4               1  ['wonderful', 'trouser', 'pant', 'love', 'fit'...\n",
       "...           ...                                                ...\n",
       "7995            0  ['want', 'love', 'kimonoit', 'beautiful', 'wor...\n",
       "7996            0  ['beautiful', 'springsummer', 'quality', 'mate...\n",
       "7997            0  ['think', 'clean', 'look', 'theoryvince', 'men...\n",
       "7998            0  ['didnt', 'realize', 'sequin', 'design', 'keep...\n",
       "7999            0  ['hei', 'hei', 'favorite', 'line', 'medium', '...\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df['lemmatized']\n",
    "y = new_df['recommended']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001    ['true', 'look', 'sexy', 'model', 'much', 'mod...\n",
       "7360    ['petite', 'know', 'buy', 'big', 'portrayed', ...\n",
       "5234    ['want', 'love', 'beautiful', 'color', 'cute',...\n",
       "7390    ['knit', 'thin', 'knit', 'shirt', 'find', 'get...\n",
       "6841    ['love', 'cloth', 'stone', 'softness', 'pretti...\n",
       "                              ...                        \n",
       "4931    ['look', 'like', 'could', 'get', 'really', 'wr...\n",
       "3264    ['much', 'cuter', 'picture', 'buy', 'c', 'thin...\n",
       "1653    ['duster', 'light', 'weight', 'good', 'inside'...\n",
       "2607    ['love', 'dressy', 'yet', 'comfy', 'perfect', ...\n",
       "2732    ['want', 'several', 'month', 'ago', 'stock', '...\n",
       "Name: lemmatized, Length: 6400, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069    ['approx', 'lb', 'dress', 'purchase', 'suggest...\n",
       "1675    ['great', 'pant', 'wedge', 'teacher', 'perfect...\n",
       "6385    ['despite', 'delicacy', 'femininity', 'pattern...\n",
       "543     ['go', 'local', 'store', 'blue', 'stock', 'see...\n",
       "3213    ['try', 'instore', 'grey', 'super', 'comfy', '...\n",
       "                              ...                        \n",
       "7716    ['purplecobalt', 'beautiful', 'feel', 'entirel...\n",
       "4766    ['wore', 'romper', 'x', 'wash', 'hem', 'shorts...\n",
       "4096    ['really', 'want', 'love', 'love', 'color', 'd...\n",
       "1595    ['usually', 'super', 'girly', 'top', 'could', ...\n",
       "5023    ['purchase', 'tan', 'love', 'however', 'loose'...\n",
       "Name: lemmatized, Length: 1600, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001    1\n",
       "7360    0\n",
       "5234    0\n",
       "7390    0\n",
       "6841    0\n",
       "       ..\n",
       "4931    0\n",
       "3264    1\n",
       "1653    1\n",
       "2607    1\n",
       "2732    1\n",
       "Name: recommended, Length: 6400, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069    1\n",
       "1675    1\n",
       "6385    0\n",
       "543     1\n",
       "3213    1\n",
       "       ..\n",
       "7716    0\n",
       "4766    0\n",
       "4096    0\n",
       "1595    1\n",
       "5023    0\n",
       "Name: recommended, Length: 1600, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6021"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = 5, ngram_range = (1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a model pipeline including the vectorizer, TfidfTransformer and classifier\n",
    "\n",
    "model_nb = Pipeline([('vect', CountVectorizer(min_df = 5, ngram_range = (1,2))),                   # ignore terms that have a document frequency lower than 5\n",
    "                   ('tfidf', TfidfTransformer()),                                                  # include unigrams and bigrams\n",
    "                   ('clf', MultinomialNB()),\n",
    "                   ])\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.831875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       790\n",
      "           1       0.86      0.80      0.83       810\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.83      0.83      0.83      1600\n",
      "weighted avg       0.83      0.83      0.83      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[685 105]\n",
      " [164 646]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8c99a7189623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnf_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtick_marks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick_marks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick_marks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(ytest, pred_y)\n",
    "cnf_matrix\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
